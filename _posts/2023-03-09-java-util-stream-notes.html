---
layout: post
title: Заметки о java.util.Stream
excerpt: Практики и тонкости использования Stream, неполное руководство
metadescription:
---

<p>
    Stream API (поток) - принципиально новый способ работы с коллекциями в Java.
    Вернее, так было во времена релиза Java 8 в далёком 2014 году,
    который принёс в классический императивный объектно-ориентированный язык программирования элементы функционального программирования.
    Ввиду масштаба нововведений сначала перед разработчиками стояла задача изучить новые подходы и наработать опыт их применения.
    После первого знакомства возник соблазн применять функциональный стиль в любой возможной ситуации,
    даже если в итоге получались сложные причудливые и никому не понятные конструкции.
    Тогда могло возникнуть обратное желание - отказаться от всех новшеств, если в них нет очевидной необходимости.
    В данной статье я опишу отдельные примеры использования Stream, которые кажутся мне наиболее интересными и показательными, чтобы
    применять Stream API эффективно, делая код лучше и избегая ошибок.
</p>

<h4>Основы и мотивация</h4>

<p>
    Лучшим введением в потоки был бы перевод
    <a href="https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html">этой обучающей статьи с сайта Oracle.</a>
    Однако, это слишком громоздко и, в целом, не нужно. Я предполагаю, что читатель уже знаком с потоками и имеет определённый опыт их применения.
    Я приведу вольный перевод основных тезисов, которые будут полезны для дальнейших примеров.
</p>

<ul>
    <li>
        Почти каждое приложение работает с коллекциями: поиск, выбор элементов, трансформации, сортировки, поиски максимальных значений, агрегации.
        Несмотря на свою значимость и вездесущность, работа с коллекциями в Java далека от идеала.
        Приходится многократно реализовывать типовые алгоритмы с использованием циклов и условных операторов.
    </li>
    <li>
        Использование многоядерной архитектуры для ускорения работы с большими коллекциями требует написания сложного многопоточного кода, подверженного ошибкам.
    </li>
    <li>
        Потоки - новая абстракция, добавленная в Java 8, призваны решить эти и многие другие проблемы. Во-первых, они позволяют писать код
        в декларативном стиле. Во-вторых, позволяют использовать параллельные вычисления без написания многопоточного кода.
    </li>
</ul>

<p>
    Также в статье сравнивается код, написанный с использованием разных подходов. Он решает задачу выборки определённых транзакций
    с последующей сортировкой и получением списка идентификаторов.
</p>

<p>
Классический подход:
{% highlight java %}
    List<Transaction> groceryTransactions = new Arraylist<>();
    for(Transaction t: transactions){
        if(t.getType() == Transaction.GROCERY){
            groceryTransactions.add(t);
        }
    }
    Collections.sort(groceryTransactions, new Comparator(){
        public int compare(Transaction t1, Transaction t2){
            return t2.getValue().compareTo(t1.getValue());
        }
    });
    List<Integer> transactionIds = new ArrayList<>();
        for(Transaction t: groceryTransactions){
            transactionsIds.add(t.getId());
        }
    } {% endhighlight %}
</p>
<p>
    Реализация с использованием потоков:
{% highlight java %}
    List<Integer> transactionsIds = transactions.stream()
        .filter(t -> t.getType() == Transaction.GROCERY)
        .sorted(comparing(Transaction::getValue).reversed())
        .map(Transaction::getId)
        .toList(); {% endhighlight %}
</p>

<h4>
    Теперь только потоки?
</h4>

<p>
    Пожалуй, всё это звучит чересчур оптимистично.
    Пример задачи и её реализации, в котором объём кода уменьшается в два-три раза достаточно искусственный.
    Использовать достоинства параллельных вычислений, избегая недостатков, таких как сложность реализации и трудноуловимые ошибки - наивно.
    Может быть это больше стремление идти в ногу со временем, сделать язык более современным?
    Java сообщество давно требовало элементов функционального программирования. Кто-то перешёл на Scala, вдохновлённый лаконичностью и выразительностью функционального кода.
    Многие стали активно применять библиотеку guava, которая позволяет написать, например, такой код:

    {% highlight java %}
    //Доступно с 12 версии, опубликованной 30 апреля 2012 года
    List<String> results =
        FluentIterable.from(database.getClientList())
        .filter(activeInLastMonthPredicate)
        .transform(Functions.toStringFunction())
        .limit(10)
        .toList();    {% endhighlight %}
    Выглядит очень знакомо. Функциональный стиль программирования сформировался уже давно и для Java разработчиков изменение именно в том, что потоки добавлены в сам язык,
    нет нужды в сторонних утилитах или библиотеках. Однако, не буду углубляться в философские рассуждения, а лучше перейду к практике.
</p>

<h4>Автозаполнение (autocomplete) для почтовых индексов</h4>
<p>
    Сформулируем следующую задачу: осуществить подсказки и проверку ввода при заполнении поля с почтовым индексом.
    Такая задача может возникнуть во многих приложениях, в которых требуется указать адрес пользователя.
    Удобным и современным решением является поиск значений "на лету" по части введенного значения и вывод короткого списка подходящий значений.
    Решать задачу будем от этапа получения данных от источника до функции, возвращающей результат поиска.
    Отображение результата пользователю не входит в требования - это может быть сайт, десктопное или мобильное приложение.
    Конечным результатом будет функция, возвращающая список индексов, начинающихся с заданного префикса, ограниченный заданным размером:

    {% highlight java %}
    List<PostIndex> findByIndexStartingWith(String prefix, int limit);    {% endhighlight %}
    Метод загрузки и хранения данных не специфицирован, выбор остаётся за разработчиком.
</p>

<p>
    Эталонный справочник почтовых индексов объектов почтовой связи можно найти в
    <a href="https://www.pochta.ru/support/database/ops">открытом доступе на сайте Почты России</a>.
    В нём чуть меньше 60 000 записей и распространяется он в одном файле в <a href="https://ru.wikipedia.org/wiki/DBF"> формате DBF</a>.
    Многие разработчики не сталкиваются с данным форматом данных, сейчас более привычным был бы какой-нибудь REST сервис.
    Для проектирования решения задачи о данном формате нужно знать следующее:
    <ul>
        <li>
            Файл состоит из заголовка и набора строк, каждая из которых содержит всю информацию о почтовом индексе.
        </li>
        <li>
            Для каждого индекса кроме самого цифрового кода присутствуют несколько текстовых полей, описывающих адрес, таких как район, город, автономная область.
        </li>
        <li>
            Файл возможно читать построчно.
            Звучит очевидно, но если все данные, например, сохранить в единый JSON объект, то извлекать данные до загрузки всего файла было бы проблематично.
        </li>
        <li>
            Данные меняются редко. Вероятно не чаще раза в сутки, а обычно раз в неделю и реже.
        </li>
    </ul>
</p>

<p>
    Стандартное решение данной задачи - загружать файл с индексами ежедневно в корпоративную базу данных, в которой индексы представлены отдельной таблицей, например, POST_INDEX.
    В этом случае выбор данных будет осуществляться простым запросом наподобие такого:
    {% highlight sql %}
        select * from POST_INDEX
            where index like '1017%'
            order by index
            limit 10 ;    {% endhighlight %}
    Базы данных очень хорошо оптимизированы, поэтому проблем с производительностью такого запроса не возникнет.
    Однако подходящей базы может не быть под рукой, она может быть уже перегружена и дорога в обслуживании.
    Кроме того, несмотря на быструю обработку запроса, все равно сетевое взаимодействие с базой данных вносит определённую задержку.
    Поэтому с учётом специфики задачи, а также стремлением попрактиковаться с потоками, выберем альтернативный подход - хранение данных в памяти JVM.
</p>

<p>
    Выделим два интерфейса для решения поставленной задачи:
    <ol>
        <li>Первый отвечает за загрузку данных из файла и преобразование к доменной модели.
            {% highlight java %}
    public interface PostIndexReader {
        //Вариант 1 - стрим
        Stream<PostIndex> readAll();
        //Вариант 2 - список
        List<PostIndex> readAllList();
        //Вариант 3 - обработчик, который будет вызван на каждый элемент если запись соответствует указанному условию
        void readAll(Predicate<? super PostIndex> acceptPostIndex,
               Consumer<? super PostIndex> handler);
        //Другие варианты, например, Visitor паттерн
        ...
    } {% endhighlight %}
        </li>
        <li> Второй - аналог DAO, отвечает за хранение и методы доступа к данным.
            Детали хранения будут выбраны в реализации, интерфейс фиксирует только контракт доступа к данным.

            {% highlight java %}
    public interface PostIndexRepository {
        List<PostIndex> findByIndexStartingWith(String prefix, int limit);
    }  {% endhighlight %}
        </li>
    </ol>
    Первый интерфейс определяет, насколько гибкой будет загрузка данных из файла. С точки зрения реализации проще всего было бы загрузить все данные в список,
    освободить файловые дескрипторы (закрыть FileInputStream) и передать управление другим компонентам.
    На данный момент это возможно, так как мы знаем, что входной файл в несжатом виде занимает 25мб на диске,
    значит если даже количество записей со временем увеличится в пять раз, мы вероятно не получим ошибки переполнения памяти.
    Ситуация изменится, если во входном файле будет сильно больше данных, но при этом нужна только небольшая их часть.
    Например, мы будем обрабатывать индексы не из российского справочника, а из международного.
    При этом по определённым флагам часть записей мы будем игнорировать из-за того,
    что данные индексы не входят в регионы доставки или являются техническими.
    Такая ситуация вполне реальна - поставщик данных формирует один полный справочник, а различные системы выбирают только нужную часть.
</p>

<p>
    Желательно спроектировать систему так, чтобы она выдерживала определённый рост нагрузки и объёма данных.
    Stream в качестве возвращаемого значение позволяет достичь сразу нескольких целей:
    <ul>
        <li>Экономия памяти, потому что не нужно формировать список всех записей исходного файла.
            Позволяет работать с файлами большого размера.</li>
        <li>Возможность не вычитывать файл целиком. Например, если нам нужен только московский регион, и он расположен в начале файла, то оставшуюся часть файла можно игнорировать.
            Достаточно при обработке потока применить метод takeWhile (например, .takeWhile(pi -> "МОСКВА".equals(pi.region());)
        </li>
        <li>
            Параллелизм. В общем да, но в данном случае нет. Если бы данные приходили из нескольких файлов, то можно было бы разделить обработку по файлам.
            Даже если обрабатывать один файл в несколько потоков и возможно теоретически, то библиотека для работы с файлами DBF точно на такое не рассчитана.
        </li>
    </ul>
</p>

<p>
    Чтобы без использования потоков достичь первой цели можно передать в качестве параметров обработчик и (необязательно)
    условие фильтрации записей (вариант 3 в листинге выше).
</p>

<p>
    Достичь одновременно первых двух целей можно с помощью паттерна Посетитель (Visitor).
    Пример этого паттерна - <a href="https://docs.oracle.com/javase/7/docs/api/java/nio/file/FileVisitor.html">обход файлового дерева</a>.
    Но для обхода строк в одном файле данный подход явно избыточен.
</p>

<p>
    Что ж, для начала реализуем чтение записей в список:
    {% highlight java %}
    @Override
    public List<PostIndex> readAllList() {
        List<PostIndex> result = new ArrayList<>();
        try (BufferedInputStream bis = new BufferedInputStream(Files.newInputStream(indexFilePath));
            DBFReader reader = new DBFReader(bis)) {//DBFReader - библиотечный класс
            //Вспомогательный класс, инкапсулирующий формат файла (порядок полей и типы)
            var postIndexFileStructure = PostIndexDbfFileStructure.fromReader(reader);

            Object[] rowObjects;
            while ((rowObjects = reader.nextRecord()) != null) {
                result.add(postIndexFileStructure.rawRecordToPostIndex(rowObjects));
            }
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        }
        return result;
    } {% endhighlight %}
    Код достаточно прямолинейный - сначала открываем FileInputStream, обрабатываем заголовок.
    Потом вычитываем записи, пока файл не будет обработан полностью, тогда reader.nextRecord() вернёт null.
    Каждую запись преобразуем к доменному объекту и добавляем в результирующий список.
    Все открытые ресурсы освобождаются автоматически при выходе из блока try-with-resources.
</p>
<p>
    Должно быть несложно преобразовать такой код так, чтобы возвращаемым результатом стал поток. Или нет?
    Давайте порассуждаем. Ради шутки можно просто получить поток из готового списка:
    {% highlight java %}

    return readAllList().stream(); //"тот же список, только в профиль"    {% endhighlight %}
    Сработает, но вся выгода от потоков сойдёт на нет.
    А какие вообще существуют способы получения потоков?
    Давайте разбираться.
</p>

<h4>
    Создание потоков
</h4>

<p>
    1. Начнём с перечисления самых простых и часто используемых способов и оценим, насколько они подходят для решения нашей задачи.
    Пустой поток можно создать вызовом Stream.of() или Stream.empty():
    {% highlight java %}

    Stream<String> emptyStream1 = Stream.of();
    Stream<String> emptyStream2 = Stream.empty();    {% endhighlight %}
</p>
<p>
    2. Поток из фиксированного количества элементов можно создать с помощью метода Stream.of(...):
    {% highlight java %}

    Stream<String> someLetters = Stream.of("a", "b", "c");    {% endhighlight %}
</p>
<p>
    3. Поток из фиксированного количества элементов можно создать также в помощью класса Stream.Builder:
    {% highlight java %}

    Stream<String> someLetters = Stream.<String>builder()
        .add("a").add("b").add("c")
        .build();    {% endhighlight %}
    или так:
    {% highlight java %}
    Stream.Builder<String> builder = Stream.builder();
    builder.accept("a");
    builder.accept("b");
    builder.accept("c");

    Stream<String> someLetters = builder.build();

    {% endhighlight %}
    Такой способ может быть альтернативой созданию потока из ArrayList. Реализация Stream.Builder использует SpinedBuffer,
    который вместо одного большого массива, содержащего все элементы использует массив массивов.
    Такая реализация не требует непрерывной области памяти, в которую бы поместились все элементы,
    что в теории упрощает управление памятью и снижает нагрузку на сборщик мусора.
    На практике такой способ создания потоков используется редко.
</p>
<p>
    4. Метод Stream.iterate(...) позволяет создавать потоки, в которых следующий элемент можно получить из предыдущего.
    Нужно задать начальный элемент и функцию получения следующего элемента (UnaryOperator).
    Дополнительно можно указать условие конца итерации.
    Например, получим поток дат (LocalDate) на 7 дней вперёд, а также поток дат до конца месяца:
    {% highlight java %}

    Stream<LocalDate> oneWeekForward = Stream.iterate(today, d -> d.plusDays(1))
        .limit(7);
    Stream<LocalDate> datesTillEndOfMonth =
        Stream.iterate(today, d -> d.getMonth() == today.getMonth(), d -> d.plusDays(1));    {% endhighlight %}

    Кажется, что можно было бы как-то применить такой способ создания потока в нашей задаче,
        ведь мы в цикле повторяем определённую операцию до наступления заданного условия.
    Но нам недостаточно отдельно взятого объекта (Object[] rowObjects) одновременно и для проверки конца итерации, и для получения следующего элемента.

    Поэтому если мы и сможем как-то воспользоваться методом Stream.iterate,
        то понадобится инкапсулировать всё необходимое в единый класс с функционалом наподобие итератора, а это уже совсем другой способ.
</p>
<p>
    Перед тем, как пойти дальше, добавлю важное замечание касательно Stream.iterate.
    Интуитивно понятно, что такой поток нельзя обработать параллельно, так как мы не можем получить следующий элемент, не обработав предыдущий.
    Программа даже не знает, нужен ли следующий элемент вообще, может быть поток надо завершить из-за наступления условия, указанного параметром в takeWhile.
    Так вот, это "интуитивное утверждение" полностью ошибочно. Работая с потоками, следует избегать предположений о том, как код скорее всего работает, а вместо
    этого аккуратно следовать спецификации, указанной в документации. Никаких указаний на то, что к потоку,
    порождённому методом Stream.iterate нельзя применить вызов функции .parallel()  нет.
    Давайте запустим такую программу, которая порождает и печатает параллельно поток чисел от 0 до 9, но дополнительно печатает информацию о каждом вызове инкремента:
    {% highlight java %}
    UnaryOperator<Integer> incrementAndPrint = (i) -> {
        System.out.println("increment " + i);
        return i + 1;
    };

    Stream.iterate(0,  incrementAndPrint)
        .limit(10)
        .parallel()
        .forEach(i -> System.out.print(i + ", "));
    {% endhighlight %}
    Программе каким-то образом удаётся выполнить итерацию параллельно и перемешать числа в выводе: "6, 2, 5, 4, 7, 3, 8, 1, 0, 9,".
    Но намного интереснее то, что перед этим будут распечатаны строки "Iterate 0", "Iterate 1", ..., "Iterate 3070".
    Чтобы обеспечить производительность параллельных вычислений, текущей реализацией JVM был сначала создан буфер элементов до числа 3070.
    Большая часть вызовов оператора инкремента была напрасной, ведь мы ограничили длину потока всего десятью элементами.
    Обнаружив такое неочевидное поведение, к использованию параллельных потоков будем подходить медленно и с опаской.
</p>

<p>
    5. Метод Stream.generate(Supplier s) позволяет создать бесконечный поток, элементы которого - результат многократного вызова переданного
    Supplier. Ограничить такой поток можно при помощи функций limit или takeWhile. Например, сгенерируем десять случайных идентификаторов.
    {% highlight java %}
    Stream<String> uuids = Stream.generate(() -> UUID.randomUUID().toString())
        .limit(10);  {% endhighlight %}
    К нашей задачи обработки почтовых индексов метод generate на первый взгляд подходит довольно хорошо.
    Идея в том, чтобы Supplier вычитывал следующую запись, а по условию takeWhile(r -> r != null) поток завершился.
    Так выглядит получившийся код, инициализация и обработка исключений опущены для краткости:
    {% highlight java %}
    return Stream.generate(() -> reader.nextRow())
        .takeWhile(r -> r != null)
        .map(postIndexRowMapper::map);
    {% endhighlight %}
    Простое выполнение данного кода происходит без ошибки и возвращает правильный результат.
    Однако стоит добавить вызов функции .parallel(), как программа завершается с исключением:
    {% highlight java %}
    postIndexreaderImpl.readAllStreamHackingGenerate().toList(); //корректно
    postIndexReaderImpl.readAllStreamHackingGenerate()
        .parallel().toList(); //исключение
    {% endhighlight %}
    {% highlight text %}

    com.linuxense.javadbf.DBFException
    : com.linuxense.javadbf.DBFException
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)
    ...
    Caused by: java.io.EOFException
        at java.base/java.io.DataInputStream.readFully(DataInputStream.java:203)
        at java.base/java.io.DataInputStream.readFully(DataInputStream.java:172)
        at com.linuxense.javadbf.DBFReader.getFieldValue(DBFReader.java:415)
        at com.linuxense.javadbf.DBFReader.nextRecord(DBFReader.java:345)
    ... 16 more
    {% endhighlight %}
</p>

<p>
    Фактическая причина ошибки и аварийного завершения программы в том, что Supplier, который передан в generate и условие завершения в
    takeWhile не обязаны вызываться строго последовательно, не допуская ни единого "лишнего" вызова Supplier. При этом поведение библиотеки
    предполагает, что после того как метод readRecord вернёт null, нужно завершить чтение данных, а при попытке будет выброшено исключение java.io.EOFException.
    Запретить клиентам нашего интерфейса применить параллельный поток невозможно.
    Кроме того, в будущих версиях JVM такой код может перестать работать и в последовательном (в противопоставление параллельному) режиме, если
    разработчики применят какие-то оптимизации.
</p>

<p>
    Принципиальная причина ошибки в том, что нарушен контракт использования метода Stream.generate(). Javadoc этого метода:
    {% highlight java %}
    @NotNull
    @Contract("_->new")
    public static <T> Stream<T> generate(
        @NotNull java.util.function.Supplier<? extends T> s)   {% endhighlight %}
    {% highlight text %}
    Returns an infinite sequential unordered stream where each element is generated by the provided Supplier.
    This is suitable for generating constant streams, streams of random elements, etc.
    @Params:    s – the Supplier of generated elements
    @Returns:    a new infinite sequential unordered Stream
    {% endhighlight %}
    Ключевое слово здесь - unordered, то есть неупорядоченный, мы не можем делать никаких предположений о порядке элементов.
    Но это не очень понятно. Простой принцип, которым следует руководствоваться - использовать только потокобезопасные компоненты без состояния.
    Это касается предикатов (параметров filter, takeWhile, dropWhile, anyMatch и других),
    функций отображения (параметров map, flatMap), параметров, передаваемых в метод peek.
    В противном случае нужно очень хорошо понимать внутреннее устройство потоков, чтобы не допустить скрытых ошибок.
</p>

<p>
    Так или иначе, с помощью generate корректно и надёжно решить поставленную задачу не получилось, пробуем другие способы.
</p>

<p>
    6. Получение потока из Iterator или Iterable. Между Iterator и Iterable разница минимальна и выбор зависит от того,
    может ли наш класс порождать итераторы (как коллекция) или же это скорее одноразовая операция.
    Главное назначение Iterable - использование в конструкции forEach. В случае когда нужен только итератор,
    Iterable добавит несколько неиспользуемых строк кода и может кого-то запутать.
</p>

<p>
    Реализуем логику получения почтовых индексов в виде итератора.
    Только не будем сразу преобразовывать строки к доменным объектам, чтобы код можно было переиспользовать для чтения произвольных DBF файлов.
    Благодаря потокам, такое преобразование легко можно будет осуществить после, добавлением вызова метод .map(...), не усложняя код итератора.

    {% highlight java %}
    public class DbfRowIterator implements Iterator<DBFRow> {
        private final DBFReader reader;

        boolean valueReady = false;
        private DBFRow nextRow;

        public DbfRowIterator(DBFReader reader) {
            this.reader = reader;
        }

        @Override
        public boolean hasNext() {
            if (!valueReady) {
                nextRow = reader.nextRow();
                valueReady = true;
            }

            return nextRow != null;
        }

        @Override
        public DBFRow next() {
            if (!valueReady && !hasNext()) {
                throw new NoSuchElementException();
            } else {
                valueReady = false;

                DBFRow row = nextRow;
                nextRow = null;
                return row;
            }
        }
    }  {% endhighlight %}
    Итераторы разработчикам давно знакомы, поэтому не буду заострять внимание на его реализации. Отмечу лишь, что приходится вводить флаг valueReady
    и вычитывать одну запись наперёд (nextRow) из-за того, что работа с файлами не предполагает идемпотентной (дающей тот же результат при многократном вызове)
    проверки наличия данных во входном потоке (InputStream). Иными словами, в классе DBFReader, нет метода isMoreRowsAvailable().
</p>

<p>
    Следующие шаги - это цепочка Iterator -> Spliterator -> Stream. На вопрос как преобразовать Iterator к Stream
    <a href="https://stackoverflow.com/questions/24511052/how-to-convert-an-iterator-to-a-stream">cамый популярный ответ на Stackoverflow такой:</a>

    {% highlight java %}
    Iterator<String> sourceIterator = Arrays.asList("A", "B", "C").iterator();
    Stream<String> targetStream = StreamSupport.stream(
        Spliterators.spliteratorUnknownSize(sourceIterator, Spliterator.ORDERED), false);
    {% endhighlight %}
    Или
    {% highlight java %}
    Iterator<String> sourceIterator = Arrays.asList("A", "B", "C").iterator();
    Stream<String> targetStream = StreamSupport.stream(
        Spliterators.spliteratorUnknownSize(sourceIterator, Spliterator.ORDERED), false);
    {% endhighlight %}
    В общем, ответ верный в обоих случаях, но не хотелось бы каждый раз отправляться за ответом на Stackoverfow.
</p>

<p>
    Вместо этого я предлагаю поступить чуть иначе - зная, что класс Iterable теперь предоставляет метод spliterator(), скопируем его код:
    {% highlight java %}
    default Spliterator<T> spliterator() {
        return Spliterators.spliteratorUnknownSize(iterator(), 0);
    }{% endhighlight %}
    А имея Spliterator получить Stream можно используя один из методов утилитного класса StreamSupport.
    В итоге получаем:
    {% highlight java %}
    public Stream<PostIndex> readAllStreamNaive() {
        try (BufferedInputStream bis = new BufferedInputStream(Files.newInputStream(indexFilePath));
            DBFReader reader = new DBFReader(bis)) {

        Iterator<DBFRow> dbfRowIterator = new DbfRowIterator(reader);
        return StreamSupport.stream(
            Spliterators.spliteratorUnknownSize(dbfRowIterator, 0), false)
                .map(postIndexRowMapper::map);
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        }
    }{% endhighlight %}
    Что же здесь происходит? Имея Iterator мы получаем Spliterator, но указываем, что никакой уточняющей информации об ограничениях,
            накладываемых на элементы внутри Iterator у нас нет.
    Можно было бы указать ORDERED или ORDERED | NONNULL | IMMUTABLE, но лучше сделать так же, как в Iterable и передать 0.
    По крайней мере, пока нет глубокого понимания, что из себя представляют все эти флаги.
    Создавая поток из Spliterator указываем параметр parallel = false. Это начальное значение флага, которое может быть изменено
    позже вызовами методов .parallel() или .sequential().
</p>

<p>
    Кстати, после первой же терминальной операции на результирующем потоке код падает с исключением:
    {% highlight text %}
    java.lang.IllegalArgumentException: this DBFReader is closed
        at com.linuxense.javadbf.DBFReader.nextRecord(DBFReader.java:312)
        at com.linuxense.javadbf.DBFReader.nextRow(DBFReader.java:404)
        at com.hipravin.post.reader.dbf.DbfRowIterator.hasNext(DbfRowIterator.java:22)
        at java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)
        at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
        at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
        at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921)
        at java.base/java.util.stream.ReduceOps$5.evaluateSequential(ReduceOps.java:258)
        at java.base/java.util.stream.ReduceOps$5.evaluateSequential(ReduceOps.java:248)
        at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.base/java.util.stream.ReferencePipeline.count(ReferencePipeline.java:709)
    {% endhighlight %}
    Опытные "стримеры", наверное, уже уловили ошибку увидев блок try-with-resources из которого возвращается поток.
    Я хотел показать, как работа с потоками так же, как и со списками, приводит к подобным ошибкам, чтобы акцентировать внимание на их особенностях.
    Что же произошло? Одно из первых утверждений о потоках - разделение промежуточных и терминальных операций.
    На готовом потоке можно указать только одну терминальную операцию (count, findFirst, anyMatch,
    collect, forEach и т.д.). Но до тех пор, пока она не вызвана, добавление промежуточных операций не приводит к началу обработки элементов потока.
    Проще говоря, ничего не происходит, пока не вызвана терминальная операция.
</p>

<p>
    Чтобы терминальная операция выполнилась успешно, необходимо, чтобы источник элементов потока в момент её выполнения был доступен.
    Когда поток получается из коллекции, достаточно, чтобы эта коллекция ещё была в памяти - это получается само собой
    благодаря сборщику мусора, который корректно отслеживает ссылки на объекты и "знает", что объект потока ссылается на, например,
    экземпляр ArrayList и не удалит его из кучи даже если других ссылок на этот экземпляр нет. Поток случайных чисел, получаемый
    методом new Random().ints() также не может каким=то образом "устареть" и сломаться из-за того, что источник элементов больше не активен.
</p>

<p>
    Другое дело - сетевые, файловые ресурсы, обработка ответов от базы данных - всё то, что надо открывать, а потом закрывать.
    При создании потока мы открываем ресурс. В задаче почтовых индексов мы открываем файл на чтение, но закрыть его явно в блоке finally или неявно в try-with-resources
    мы не можем, потому что данные понадобятся позже. Прочитать файл до конца, а потом закрыть тоже не получится, так как это противоречит изначальной задумке использования потоков.
    Выход заключается в том, чтобы обязать клиента закрыть открытые ресурсы. Происходит это следующим образом:
    <ol>
        <li>
            Одна из промежуточных операций при работе с потоками - onClose(Runnable closeHandler) позволяет добавить обработчик закрытия ресурсов после завершения работы с потоком.
        </li>
        <li>
            Интерфейс Stream расширяет интрерфейс AutoCloseable, соответственно все обработчики из onClose будут вызвани при вызове close() на потоке.
        </li>
        <li>
            Потоки, которые требуют освобождения ресурсов, необходимо использовать в блоке try-with-resources:
            {% highlight java %}
    try(Stream<PostIndex> postIndexStream = postIndexReader.readAll()) {
        long postIndicesCount = postIndexStream.count();
        assertEquals(58444, postIndicesCount);
    }
            {% endhighlight %}
        </li>
    </ol>
    Важно понимать, что терминальные операции не приводят к вызову метода close(), что было бы логично на первый взгляд.
    Но, во-первых, ресурсы нужно освобождать даже если поток остался невостребованным.
    Во-вторых, в процессе обработки элементов потока может возникнуть исключение.
</p>

<p>
    Окончательный вариант получения потока почтовых индексов, вычитанных из файла, используя итератор:
    {% highlight java %}
    public Stream<PostIndex> readAll() {
        try {
            BufferedInputStream bis = new BufferedInputStream(Files.newInputStream(indexFilePath));
            DBFReader reader = new DBFReader(bis);

            Iterator<DBFRow> dbfRowIterator = new DbfRowIterator(reader);
            return StreamSupport.stream(
                Spliterators.spliteratorUnknownSize(dbfRowIterator, 0), false)
                    .map(postIndexRowMapper::map)
                    .onClose(() -> closeQuietly(bis))
                    .onClose(() -> closeQuietly(reader));
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        }
    }

    {% endhighlight %}
</p>

<h3>Заключение</h3>
<p>
    TODO:
</p>



============
<p>
    6*. Следующий способ - написать собственную реализацию Stream, включая методы filter, map, reduce, collect, findFirst и прочие.
    Шучу такого способа не существует. Так поступают только разработчики библиотек, расширяющих функционал потоков, например, библиотеки
    <a href="https://github.com/amaembo/streamex">StreamEx</a>.
    Реализация потоков слишком сложная и скрыта для какого-либо расширения. Нет такого класса как,
    например, AbstractList, чтобы унаследовать его и немного подправить поведение потоков под свои нужды, не реализуя весь функционал практически с нуля.
    Но нам и не требуется никаких изменений в большинстве операций, нужно только описать источник элементов потока, который получает элементы их из файла и завершается,
    когда встречает null.
    Правильный способ - реализовать один из интерфейсов: Iterable, Iterator или Spliterator.
</p>

============

Spliterator в свою очередь - это специальный итератор для потоков. Он позволяет управлять поведением параллельных потоков с помощью метода trySplit.
Но чтобы эффективно использовать эти возможности должен существовать способ разделить генерируемый поток элементов на два приблизительно равных потока.
А каждую из получившихся частей - ещё на два и так далее. Это хорошо срабатывает для коллекций известного размера наподобие ArrayList.
Для задачи почтовых индексов мы не знаем количества записей в файле, а также не имеем способа как-то разделить эти записи на части до того, как достигнем конца файла.

Если попытаться в подобном случае реализовать Spliterator, то метод trySplit придётся оставить с вводящей в заблуждение конструкцией return null:
{% highlight java %}
@Override
public Spliterator<E> trySplit(){
    //в отсутствии комментария выглядит как код, который генерирует IDE
    //для того, чтобы класс скомпилировался
    return null;
    }
    {% endhighlight %}
    Для Spliterator также нужно реализовать метод, возвращающий количество элементов потока и какие-то характеристики:
    {% highlight java %}
    @Override
    public long estimateSize(){
    //и тут какая-то магическая константа
    return Long.MAX_VALUE;
    }
    @Override
    public int characteristics(){
    //просто 0, очень осмысленно
    return 0;
    }
    {% endhighlight %}



    ==============
https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html

-3 Начать с примера autocomplete почтовых индексов
https://www.pochta.ru/support/database/ops

-2. Самый типовой и распространённый случай
        stream
        filter
        filter
        collect toList

-1 Накладные расходы или насколько стримы медленные



0. Полезные стримы:
    SplittableRandom
    FileWalk...?
    Spring Data

001 forEach forEachOrdered

01 Оптимизация стрима - несколько filter, map в один - имеет ли смысл?


1. Необходимость закрывать Stream
   Files.lines

    Какие ещё стримы надо закрывать
    https://docs.spring.io/spring-data/jpa/docs/current/reference/html/
    A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example:

JPA getResultStream
hibernate-core 5.6.12-Final
org.hibernate.internal.AbstractScrollableResults
@Override
public final void close() {
if ( this.closed ) {
// noop if already closed
return;
}

// not absolutely necessary, but does help with aggressive release
//session.getJDBCContext().getConnectionManager().closeQueryStatement( ps, resultSet );
final JdbcCoordinator jdbcCoordinator = session.getJdbcCoordinator();
jdbcCoordinator.getResourceRegistry().release( ps );
jdbcCoordinator.afterStatementExecution();
try {
session.getPersistenceContextInternal().getLoadContexts().cleanup( resultSet );
}
catch (Throwable ignore) {
// ignore this error for now
if ( LOG.isTraceEnabled() ) {
LOG.tracev( "Exception trying to cleanup load context : {0}", ignore.getMessage() );
}
}

this.closed = true;
}


2. Spring data returning Stream - transaction

3. Стримы изнутри - Reference Pipeline ?

5. Параллельные вычисления, ordering /sorting
https://developer.ibm.com/articles/j-java-streams-3-brian-goetz/#eo
If the stream does have an encounter order, most stream operations must respect that order. For sequential executions, preserving encounter order is essentially free, because elements are naturally processed in the order in which they're encountered. Even in parallel, for many operations (stateless intermediate operations and certain terminal operations such as reduce()), respecting the encounter order doesn't impose any real costs. But for others (stateful intermediate operations, and terminal operations whose semantics are tied to encounter order, such as findFirst() or forEachOrdered()), the obligation to respect the encounter order in a parallel execution can be significant. If the stream has a defined encounter order, but that order isn't significant to the result, it might be possible to speed up parallel execution of pipelines containing order-sensitive operations by removing the ORDERED flag with the unordered() operation.

6. Создание своих стримов
    -из итератора
    -трай адванс

7. Сравнение кода со стримами и без

8. Много стандартных примеров

9 Промежуточные limit ?

10. firstMatch vs anyMatch

11. toMap without merge strategy

12. IntStream и прочие там где можно вместо операций с объектами, производительность

13. mapMulti examples and performance
        (words - letters)

статистика букв в словах длиннее трёх букв

14 zip
https://dzone.com/articles/bridge-the-gap-of-zip-operation

15. method rerefence or not??

16 teeing

17 переход на стримы внутри проекта в качестве возвращаемого значения

18 toList , Collectors.toList

19 iterate and parallel
Stream<Long> literate = Stream.iterate(1L, l -> l + 1)
    .limit(100)
    .peek(l -> System.out.println(Thread.currentThread().getName() + " - " + l + " peek"))
    .limit(50)
    .sorted()
    .parallel(); //batch (1024), ArrayListSpliterator

20 аргумент конструктора лучше limit

21 Iterable.spliterator вместо стековерфлоу

====
To summarize what we’ve learned so far, working with streams, in general, involves three things:

A datasource (such as a collection) on which to perform a query
A chain of intermediate operations, which form a stream pipeline
One terminal operation, which executes the stream pipeline and produces a result






Benchmark                                     (fileName)   Mode  Cnt      Score     Error  Units
BenchmarkConfig.benchBufferedReader      sample-tiny.txt  thrpt   25  14053,105 ± 327,748  ops/s
BenchmarkConfig.benchFileLines           sample-tiny.txt  thrpt   25  17067,844 ± 269,158  ops/s
BenchmarkConfig.benchReadAllLines        sample-tiny.txt  thrpt   25  15002,664 ± 240,188  ops/s
BenchmarkConfig.benchWithoutProperClose  sample-tiny.txt  thrpt   25   8510,614 ± 425,730  ops/s


Benchmark                                  (fileName)  (minWordLength)   Mode  Cnt    Score    Error  Units
BenchmarkConfig.imperative      voyna-i-mir-tom-1.txt                3  thrpt    9   11,762 ±  0,924  ops/s
BenchmarkConfig.readFileFully   voyna-i-mir-tom-1.txt                3  thrpt    9  160,022 ± 21,510  ops/s
BenchmarkConfig.streamFlatMap   voyna-i-mir-tom-1.txt                3  thrpt    9   10,966 ±  1,056  ops/s
BenchmarkConfig.streamMapMulti  voyna-i-mir-tom-1.txt                3  thrpt    9   10,941 ±  1,703  ops/s

Benchmark                                  (fileName)  (minWordLength)   Mode  Cnt    Score    Error  Units
BenchmarkConfig.imperative      voyna-i-mir-tom-1.txt                3  thrpt    9   13,847 ±  1,753  ops/s
BenchmarkConfig.readFileFully   voyna-i-mir-tom-1.txt                3  thrpt    9  188,147 ± 33,165  ops/s
BenchmarkConfig.streamFlatMap   voyna-i-mir-tom-1.txt                3  thrpt    9   12,424 ±  2,093  ops/s
BenchmarkConfig.streamMapMulti  voyna-i-mir-tom-1.txt                3  thrpt    9   13,087 ±  1,238  ops/s
BenchmarkConfig.sum3FlatMap                       N/A              N/A  thrpt    9    2,034 ±  0,303  ops/s
BenchmarkConfig.sum3MapMulti                      N/A              N/A  thrpt    9    4,544 ±  0,511  ops/s
BenchmarkConfig.sum3MapMulti2                     N/A              N/A  thrpt    9    4,579 ±  0,830  ops/s