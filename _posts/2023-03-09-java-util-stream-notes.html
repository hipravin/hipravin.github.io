---
layout: post
title: Заметки о java.util.Stream
excerpt: Практики и тонкости использования Stream, неполное руководство
metadescription:
---

<p>
    Stream API (поток) - принципиально новый способ работы с коллекциями в Java.
    Вернее, так было во времена релиза Java 8 в далёком 2014 году,
    который принёс в классический императивный объектно-ориентированный язык программирования элементы функционального программирования.
    Ввиду масштаба нововведений сначала перед разработчиками стояла задача изучить новые подходы и наработать опыт их применения.
    После первого знакомства возник соблазн применять функциональный стиль в любой возможной ситуации,
    даже если в итоге получались сложные причудливые и никому не понятные конструкции.
    Тогда могло возникнуть обратное желание - отказаться от всех новшеств, если в них нет очевидной необходимости.
    В данной статье я опишу отдельные примеры использования Stream, которые кажутся мне наиболее интересными и показательными, чтобы
    применять Stream API эффективно, делая код лучше и избегая ошибок.
</p>

<h4>Основы и мотивация</h4>

<p>
    Лучшим введением в потоки был бы перевод
    <a href="https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html">этой обучающей статьи с сайта Oracle.</a>
    Однако, это слишком громоздко и, в целом, не нужно. Я предполагаю, что читатель уже знаком с потоками и имеет определённый опыт их применения.
    Я приведу вольный перевод основных тезисов, которые будут полезны для дальнейших примеров.
</p>

<ul>
    <li>
        Почти каждое приложение работает с коллекциями: поиск, выбор элементов, трансформации, сортировки, поиски максимальных значений, агрегации.
        Несмотря на свою значимость и вездесущность, работа с коллекциями в Java далека от идеала.
        Приходится многократно реализовывать типовые алгоритмы с использованием циклов и условных операторов.
    </li>
    <li>
        Использование многоядерной архитектуры для ускорения работы с большими коллекциями требует написания сложного многопоточного кода, подверженного ошибкам.
    </li>
    <li>
        Потоки - новая абстракция, добавленная в Java 8, призваны решить эти и многие другие проблемы. Во-первых, они позволяют писать код
        в декларативном стиле. Во-вторых, позволяют использовать параллельные вычисления без написания многопоточного кода.
    </li>
</ul>

<p>
    Также в статье сравнивается код, написанный с использованием разных подходов. Он решает задачу выборки определённых транзакций
    с последующей сортировкой и получением списка идентификаторов.
</p>

<p>
Классический подход:
{% highlight java %}
    List<Transaction> groceryTransactions = new Arraylist<>();
    for(Transaction t: transactions){
        if(t.getType() == Transaction.GROCERY){
            groceryTransactions.add(t);
        }
    }
    Collections.sort(groceryTransactions, new Comparator(){
        public int compare(Transaction t1, Transaction t2){
            return t2.getValue().compareTo(t1.getValue());
        }
    });
    List<Integer> transactionIds = new ArrayList<>();
        for(Transaction t: groceryTransactions){
            transactionsIds.add(t.getId());
        }
    } {% endhighlight %}
</p>
<p>
    Реализация с использованием потоков:
{% highlight java %}
    List<Integer> transactionsIds = transactions.stream()
        .filter(t -> t.getType() == Transaction.GROCERY)
        .sorted(comparing(Transaction::getValue).reversed())
        .map(Transaction::getId)
        .toList(); {% endhighlight %}
</p>

<h4>
    Теперь только потоки?
</h4>

<p>
    Пожалуй, всё это звучит чересчур оптимистично.
    Пример задачи и её реализации, в котором объём кода уменьшается в два-три раза достаточно искусственный.
    Использовать достоинства параллельных вычислений, избегая недостатков, таких как сложность реализации и трудноуловимые ошибки - наивно.
    Может быть это больше стремление идти в ногу со временем, сделать язык более современным?
    Java сообщество давно требовало элементов функционального программирования. Кто-то перешёл на Scala, вдохновлённый лаконичностью и выразительностью функционального кода.
    Многие стали активно применять библиотеку guava, которая позволяет написать, например, такой код:

    {% highlight java %}
    //Доступно с 12 версии, опубликованной 30 апреля 2012 года
    List<String> results =
        FluentIterable.from(database.getClientList())
        .filter(activeInLastMonthPredicate)
        .transform(Functions.toStringFunction())
        .limit(10)
        .toList();    {% endhighlight %}
    Выглядит очень знакомо. Функциональный стиль программирования сформировался уже давно и для Java разработчиков изменение именно в том, что потоки добавлены в сам язык,
    нет нужды в сторонних утилитах или библиотеках. Однако, не буду углубляться в философские рассуждения, а лучше перейду к практике.
</p>

<h4>Автозаполнение (autocomplete) для почтовых индексов</h4>
<p>
    Сформулируем следующую задачу: осуществить подсказки и проверку ввода при заполнении поля с почтовым индексом.
    Такая задача может возникнуть во многих приложениях, в которых требуется указать адрес пользователя.
    Удобным и современным решением является поиск значений "на лету" по части введенного значения и вывод короткого списка подходящий значений.
    Решать задачу будем от этапа получения данных от источника до функции, возвращающей результат поиска.
    Отображение результата пользователю не входит в требования - это может быть сайт, десктопное или мобильное приложение.
    Конечным результатом будет функция, возвращающая список индексов, начинающихся с заданного префикса, ограниченный заданным размером:

    {% highlight java %}
    List<PostIndex> findByIndexStartingWith(String prefix, int limit);    {% endhighlight %}
    Метод загрузки и хранения данных не специфицирован, выбор остаётся за разработчиком.
</p>

<p>
    Эталонный справочник почтовых индексов объектов почтовой связи можно найти в
    <a href="https://www.pochta.ru/support/database/ops">открытом доступе на сайте Почты России</a>.
    В нём чуть меньше 60 000 записей и распространяется он в одном файле в <a href="https://ru.wikipedia.org/wiki/DBF"> формате DBF</a>.
    Многие разработчики не сталкиваются с данным форматом данных, сейчас более привычным был бы какой-нибудь REST сервис.
    Для проектирования решения задачи о данном формате нужно знать следующее:
    <ul>
        <li>
            Файл состоит из заголовка и набора строк, каждая из которых содержит всю информацию о почтовом индексе.
        </li>
        <li>
            Для каждого индекса кроме самого цифрового кода присутствуют несколько текстовых полей, описывающих адрес, таких как район, город, автономная область.
        </li>
        <li>
            Файл возможно читать построчно.
            Звучит очевидно, но если все данные, например, сохранить в единый JSON объект, то извлекать данные до загрузки всего файла было бы проблематично.
        </li>
        <li>
            Данные меняются редко. Вероятно не чаще раза в сутки, а обычно раз в неделю и реже.
        </li>
    </ul>
</p>

<p>
    Стандартное решение данной задачи - загружать файл с индексами ежедневно в корпоративную базу данных, в которой индексы представлены отдельной таблицей, например, POST_INDEX.
    В этом случае выбор данных будет осуществляться простым запросом наподобие такого:
    {% highlight sql %}
        select * from POST_INDEX
            where index like '1017%'
            order by index
            limit 10 ;    {% endhighlight %}
    Базы данных очень хорошо оптимизированы, поэтому проблем с производительностью такого запроса не возникнет.
    Однако подходящей базы может не быть под рукой, она может быть уже перегружена и дорога в обслуживании.
    Кроме того, несмотря на быструю обработку запроса, все равно сетевое взаимодействие с базой данных вносит определённую задержку.
    Поэтому с учётом специфики задачи, а также стремлением попрактиковаться с потоками, выберем альтернативный подход - хранение данных в памяти JVM.
</p>

<p>
    Выделим два интерфейса для решения поставленной задачи:
    <ol>
        <li>Первый отвечает за загрузку данных из файла и преобразование к доменной модели.
            {% highlight java %}
    public interface PostIndexReader {
        //Вариант 1 - стрим
        Stream<PostIndex> readAll();
        //Вариант 2 - список
        List<PostIndex> readAllList();
        //Вариант 3 - обработчик, который будет вызван на каждый элемент если запись соответствует указанному условию
        void readAll(Predicate<? super PostIndex> acceptPostIndex,
               Consumer<? super PostIndex> handler);
        //Другие варианты, например, Visitor паттерн
        ...
    } {% endhighlight %}
        </li>
        <li> Второй - аналог DAO, отвечает за хранение и методы доступа к данным.
            Детали хранения будут выбраны в реализации, интерфейс фиксирует только контракт доступа к данным.

            {% highlight java %}
    public interface PostIndexRepository {
        List<PostIndex> findByIndexStartingWith(String prefix, int limit);
    }  {% endhighlight %}
        </li>
    </ol>
    Первый интерфейс определяет, насколько гибкой будет загрузка данных из файла. С точки зрения реализации проще всего было бы загрузить все данные в список,
    освободить файловые дескрипторы (закрыть FileInputStream) и передать управление другим компонентам.
    На данный момент это возможно, так как мы знаем, что входной файл в несжатом виде занимает 25мб на диске,
    значит если даже количество записей со временем увеличится в пять раз, мы вероятно не получим ошибки переполнения памяти.
    Ситуация изменится, если во входном файле будет сильно больше данных, но при этом нужна только небольшая их часть.
    Например, мы будем обрабатывать индексы не из российского справочника, а из международного.
    При этом по определённым флагам часть записей мы будем игнорировать из-за того,
    что данные индексы не входят в регионы доставки или являются техническими.
    Такая ситуация вполне реальна - поставщик данных формирует один полный справочник, а различные системы выбирают только нужную часть.
</p>

<p>
    Желательно спроектировать систему так, чтобы она выдерживала определённый рост нагрузки и объёма данных.
    Stream в качестве возвращаемого значение позволяет достичь сразу нескольких целей:
    <ul>
        <li>Экономия памяти, потому что не нужно формировать список всех записей исходного файла.
            Позволяет работать с файлами большого размера.</li>
        <li>Возможность не вычитывать файл целиком. Например, если нам нужен только московский регион, и он расположен в начале файла, то оставшуюся часть файла можно игнорировать.
            Достаточно при обработке потока применить метод takeWhile (например, .takeWhile(pi -> "МОСКВА".equals(pi.region());)
        </li>
        <li>
            Параллелизм. В общем да, но в данном случае нет. Если бы данные приходили из нескольких файлов, то можно было бы разделить обработку по файлам.
            Даже если обрабатывать один файл в несколько потоков и возможно теоретически, то библиотека для работы с файлами DBF точно на такое не рассчитана.
        </li>
    </ul>
</p>

<p>
    Чтобы без использования потоков достичь первой цели можно передать в качестве параметров обработчик и (необязательно)
    условие фильтрации записей (вариант 3 в листинге выше).
</p>

<p>
    Достичь одновременно первых двух целей можно с помощью паттерна Посетитель (Visitor).
    Пример этого паттерна - <a href="https://docs.oracle.com/javase/7/docs/api/java/nio/file/FileVisitor.html">обход файлового дерева</a>.
    Но для обхода строк в одном файле данный подход явно избыточен.
</p>

<p>
    Что ж, для начала реализуем чтение записей в список:
    {% highlight java %}
    @Override
    public List<PostIndex> readAllList() {
        List<PostIndex> result = new ArrayList<>();
        try (BufferedInputStream bis = new BufferedInputStream(Files.newInputStream(indexFilePath));
            DBFReader reader = new DBFReader(bis)) {//DBFReader - библиотечный класс
            //Вспомогательный класс, инкапсулирующий формат файла (порядок полей и типы)
            var postIndexFileStructure = PostIndexDbfFileStructure.fromReader(reader);

            Object[] rowObjects;
            while ((rowObjects = reader.nextRecord()) != null) {
                result.add(postIndexFileStructure.rawRecordToPostIndex(rowObjects));
            }
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        }
        return result;
    } {% endhighlight %}
    Код достаточно прямолинейный - сначала открываем FileInputStream, обрабатываем заголовок.
    Потом вычитываем записи, пока файл не будет обработан полностью, тогда reader.nextRecord() вернёт null.
    Каждую запись преобразуем к доменному объекту и добавляем в результирующий список.
    Все открытые ресурсы освобождаются автоматически при выходе из блока try-with-resources.
</p>
<p>
    Должно быть несложно преобразовать такой код так, чтобы возвращаемым результатом стал поток. Или нет?
    Давайте порассуждаем. Ради шутки можно просто получить поток из готового списка:
    {% highlight java %}

    return readAllList().stream(); //"тот же список, только в профиль"    {% endhighlight %}
    Сработает, но вся выгода от потоков сойдёт на нет.
    А какие вообще существуют способы получения потоков?
    Давайте разбираться.
</p>

<h4>
    Создание потоков
</h4>

<p>
    1. Начнём с перечисления самых простых и часто используемых способов и оценим, насколько они подходят для решения нашей задачи.
    Пустой поток можно создать вызовом Stream.of() или Stream.empty().
    {% highlight java %}

    Stream<String> emptyStream1 = Stream.of();
    Stream<String> emptyStream2 = Stream.empty();    {% endhighlight %}
</p>
<p>
    2. Поток из фиксированного количества элементов можно создать с помощью метода Stream.of(...).
    {% highlight java %}

    Stream<String> someLetters = Stream.of("a", "b", "c");    {% endhighlight %}
</p>
<p>
    3. Метод Stream.iterate(...) позволяет создавать потоки, в которых следующий элемент можно получить из предыдущего.
    Нужно задать начальный элемент и функцию получения следующего элемента (UnaryOperator).
    Дополнительно можно указать условие конца итерации.
    Например, получим поток дат (LocalDate) на 7 дней вперёд, а также поток дат до конца месяца:
    {% highlight java %}

    Stream<LocalDate> oneWeekForward = Stream.iterate(today, d -> d.plusDays(1))
        .limit(7);
    Stream<LocalDate> datesTillEndOfMonth =
        Stream.iterate(today, d -> d.getMonth() == today.getMonth(), d -> d.plusDays(1));    {% endhighlight %}

    Кажется, что можно было бы как-то применить такой способ создания потока в нашей задаче,
        ведь мы в цикле повторяем определённую операцию до наступления заданного условия.
    Но нам недостаточно отдельно взятого объекта (Object[] rowObjects) одновременно и для проверки конца итерации, и для получения следующего элемента.

    Поэтому если мы и сможем как-то воспользоваться методом Stream.iterate,
        то понадобится инкапсулировать всё необходимое в единый класс с функционалом наподобие итератора, а это уже совсем другой способ.
</p>
<p>
    Перед тем, как пойти дальше, добавлю важное замечание касательно Stream.iterate.
    Интуитивно понятно, что такой поток нельзя обработать параллельно, так как мы не можем получить следующий элемент, не обработав предыдущий.
    Программа даже не знает, нужен ли следующий элемент вообще, может быть поток надо завершить из-за наступления условия, указанного параметром в takeWhile.
    Так вот, это "интуитивное утверждение" полностью ошибочно. Работая с потоками следует избегать предположений о том, как код скорее всего работает, а вместо
    этого аккуратно следовать спецификации, указанной в документации. Никаких указаний на то, что к потоку,
    порождённому методом Stream.iterate нельзя применить вызов функции .parallel()  нет.
    Давайте запустим такую программу:
    {% highlight java %}
    UnaryOperator<Integer> incrementAndPrint = (i) -> {
        System.out.println("increment " + i);
        return i + 1;
    };

    Stream.iterate(0,  incrementAndPrint)
        .limit(10)
        .parallel()
        .forEach(i -> System.out.print(i + ", "));
    {% endhighlight %}
    Программе каким-то образом удаётся выполнить итерацию параллельно и перемешать числа в выводе: "6, 2, 5, 4, 7, 3, 8, 1, 0, 9,".
    Но намного интереснее то, что перед этим будут распечатаны строки "Iterate 0", "Iterate 1", ..., "Iterate 3070".
    Чтобы обеспечить производительность параллельных вычислений, текущей реализацией JVM был сначала создан буфер элементов до числа 3070.
    Большая часть вызовов оператора инкремента была напрасной, ведь мы ограничили длину потока всего десятью элементами.
    Обнаружив такое неочевидное поведение, к использованию параллельных потоков будем подходить медленно и с опаской.
</p>

<p>
    4. Метод Stream.generate(Supplier s) позволяет создать бесконечные потоки, элементы которого - результат многократного вызова переданного
    Supplier. Ограничить такой поток можно при помощи функций limit или takeWhile. Например, сгенерируем десять случайных идентификаторов.
    {% highlight java %}
    Stream<String> uuids = Stream.generate(() -> UUID.randomUUID().toString())
        .limit(10);
       {% endhighlight %}
</p>



<h3>Заключение</h3>
<p>
    TODO:
</p>



============

==============
https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html

-3 Начать с примера autocomplete почтовых индексов
https://www.pochta.ru/support/database/ops

-2. Самый типовой и распространённый случай
        stream
        filter
        filter
        collect toList

-1 Накладные расходы или насколько стримы медленные



0. Полезные стримы:
    SplittableRandom
    FileWalk...?
    Spring Data

001 forEach forEachOrdered

01 Оптимизация стрима - несколько filter, map в один - имеет ли смысл?


1. Необходимость закрывать Stream
   Files.lines

    Какие ещё стримы надо закрывать
    https://docs.spring.io/spring-data/jpa/docs/current/reference/html/
    A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example:

JPA getResultStream
hibernate-core 5.6.12-Final
org.hibernate.internal.AbstractScrollableResults
@Override
public final void close() {
if ( this.closed ) {
// noop if already closed
return;
}

// not absolutely necessary, but does help with aggressive release
//session.getJDBCContext().getConnectionManager().closeQueryStatement( ps, resultSet );
final JdbcCoordinator jdbcCoordinator = session.getJdbcCoordinator();
jdbcCoordinator.getResourceRegistry().release( ps );
jdbcCoordinator.afterStatementExecution();
try {
session.getPersistenceContextInternal().getLoadContexts().cleanup( resultSet );
}
catch (Throwable ignore) {
// ignore this error for now
if ( LOG.isTraceEnabled() ) {
LOG.tracev( "Exception trying to cleanup load context : {0}", ignore.getMessage() );
}
}

this.closed = true;
}


2. Spring data returning Stream - transaction

3. Стримы изнутри - Reference Pipeline ?

5. Параллельные вычисления, ordering /sorting
https://developer.ibm.com/articles/j-java-streams-3-brian-goetz/#eo
If the stream does have an encounter order, most stream operations must respect that order. For sequential executions, preserving encounter order is essentially free, because elements are naturally processed in the order in which they're encountered. Even in parallel, for many operations (stateless intermediate operations and certain terminal operations such as reduce()), respecting the encounter order doesn't impose any real costs. But for others (stateful intermediate operations, and terminal operations whose semantics are tied to encounter order, such as findFirst() or forEachOrdered()), the obligation to respect the encounter order in a parallel execution can be significant. If the stream has a defined encounter order, but that order isn't significant to the result, it might be possible to speed up parallel execution of pipelines containing order-sensitive operations by removing the ORDERED flag with the unordered() operation.

6. Создание своих стримов
    -из итератора
    -трай адванс

7. Сравнение кода со стримами и без

8. Много стандартных примеров

9 Промежуточные limit ?

10. firstMatch vs anyMatch

11. toMap without merge strategy

12. IntStream и прочие там где можно вместо операций с объектами, производительность

13. mapMulti examples and performance
        (words - letters)

статистика букв в словах длиннее трёх букв

14 zip
https://dzone.com/articles/bridge-the-gap-of-zip-operation

15. method rerefence or not??

16 teeing

17 переход на стримы внутри проекта в качестве возвращаемого значения

18 toList , Collectors.toList

19 iterate and parallel
Stream<Long> literate = Stream.iterate(1L, l -> l + 1)
    .limit(100)
    .peek(l -> System.out.println(Thread.currentThread().getName() + " - " + l + " peek"))
    .limit(50)
    .sorted()
    .parallel(); //batch (1024), ArrayListSpliterator

20 аргумент конструктора лучше limit

21 Iterable.spliterator вместо стековерфлоу

====
To summarize what we’ve learned so far, working with streams, in general, involves three things:

A datasource (such as a collection) on which to perform a query
A chain of intermediate operations, which form a stream pipeline
One terminal operation, which executes the stream pipeline and produces a result






Benchmark                                     (fileName)   Mode  Cnt      Score     Error  Units
BenchmarkConfig.benchBufferedReader      sample-tiny.txt  thrpt   25  14053,105 ± 327,748  ops/s
BenchmarkConfig.benchFileLines           sample-tiny.txt  thrpt   25  17067,844 ± 269,158  ops/s
BenchmarkConfig.benchReadAllLines        sample-tiny.txt  thrpt   25  15002,664 ± 240,188  ops/s
BenchmarkConfig.benchWithoutProperClose  sample-tiny.txt  thrpt   25   8510,614 ± 425,730  ops/s


Benchmark                                  (fileName)  (minWordLength)   Mode  Cnt    Score    Error  Units
BenchmarkConfig.imperative      voyna-i-mir-tom-1.txt                3  thrpt    9   11,762 ±  0,924  ops/s
BenchmarkConfig.readFileFully   voyna-i-mir-tom-1.txt                3  thrpt    9  160,022 ± 21,510  ops/s
BenchmarkConfig.streamFlatMap   voyna-i-mir-tom-1.txt                3  thrpt    9   10,966 ±  1,056  ops/s
BenchmarkConfig.streamMapMulti  voyna-i-mir-tom-1.txt                3  thrpt    9   10,941 ±  1,703  ops/s

Benchmark                                  (fileName)  (minWordLength)   Mode  Cnt    Score    Error  Units
BenchmarkConfig.imperative      voyna-i-mir-tom-1.txt                3  thrpt    9   13,847 ±  1,753  ops/s
BenchmarkConfig.readFileFully   voyna-i-mir-tom-1.txt                3  thrpt    9  188,147 ± 33,165  ops/s
BenchmarkConfig.streamFlatMap   voyna-i-mir-tom-1.txt                3  thrpt    9   12,424 ±  2,093  ops/s
BenchmarkConfig.streamMapMulti  voyna-i-mir-tom-1.txt                3  thrpt    9   13,087 ±  1,238  ops/s
BenchmarkConfig.sum3FlatMap                       N/A              N/A  thrpt    9    2,034 ±  0,303  ops/s
BenchmarkConfig.sum3MapMulti                      N/A              N/A  thrpt    9    4,544 ±  0,511  ops/s
BenchmarkConfig.sum3MapMulti2                     N/A              N/A  thrpt    9    4,579 ±  0,830  ops/s