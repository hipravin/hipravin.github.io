---
layout: post
title: Алгоритмическая сложность
excerpt: Теория, основные алгоритмы, типовые операции с коллекциями, что спрашивают на собеседованиях
---
<p>
    Алгоритмическая сложность - тема отдельного курса в классических университетских программах. Как, впрочем, и математический анализ.
    Однако на собеседовании Вас врядли попросят посчитать интеграл, а вот про сложность алгоритмов спрашивают практически всех на любую позицию программиста.
    А знакомо ли Вам понятие алгоритмическая сложность? А какая сложность быстрой сортировки? А это в лучшем или в худшем случае?
    Бывает, с горем пополам разобрались с какой-нибудь задачей на логику, но вас добивают: а какая сложность Вашего решения, можно ли ускорить?
    В этом посте постараюсь объяснить, что тут к чему, и не пугайтесь, если для Вас NP-полнота значит не больше, чем NP-пустота.
</p>

<p>
    Важность понятия алгоритмической сложности в том, что на практике, как правило, существует разумно оптимальный способ решить задачу.
    Использовав его, дальшейшая оптимизация будет улучшать производительность на 10-50%, что потребует ещё в два или пять раз больше времени на разработку.
    Одновременно с этим, неправильно выбранный способ решения будет работать медленнее в десять, в сто, в тысячу раз. На практике система виснет на простых операциях,
    загрузка занимает секунды, а всевозможные запланированные задачи запускаются по ночам и работают часами.
    И объёмы данных отнюдь не терабайты, достаточно нескольких миллионов записей. И я расскажу, почему.
</p>

<p>Сложность алгоритма - это ответ на вопрос сколько действий придётся совершить, чтобы решить задачу, в зависимости от параметров задачи.
    Разнородных парметров в задаче можнт быть много, и разнородных действий тоже. И то и другое нужно сократить до одного: один параметр и одно действие, остальное отбросить.
    Например, Вы листаете договор из тысячи страниц и подписываете каждую. Входной параметр такой задачи будет количество страниц, а действие - поставить подпись.
    Перелистывание страниц мы не считаем за важное действие, потому что это намного быстрее подписи.
    Не важно, сколько времени уйдёт на всё, два часа или два дня - кто-то расписывается быстрее, кто-то медленнее. Но каждому потребуется 1000 действий для выполнения алгоритма.
    Если обобщить, то требуется N действий при N страницах. Легко можно посчитать, что если Вы подписали договор из 10 страницы за 10 минут, то на 60 страниц Вы потратите час.
    Если оптимизировать процесс и подписывать только первую страницу, то потребуется одна минута вне зависимости от количества страниц.
</p>

<p>
    Другой пример - на бумаге разлинован квадрат со стороной 10. В каждой клетке нужно поставить точку, всего точек 100 (N^2). Если потребовалось 10 минут на всё про всё, то,
    чтобы повторить алгоритм на квадрате со стороной 60, потребуется 10 / 100 * 60 * 60 = 360 минут, то есть 6 часов. Итого в одной задаче увеличив входной параметр в шесть раз,
    мы потратили в 6 раз больше времени, а в другой - в 36. Разница колоссальная.
</p>

<p>Я привёл три примера сложности алгоритма: константную, порядка N и порядка N^2. У программистов принято говорить "О большое от". Ниже для порядка я приведу строгое определение,
    но на практике требуется лишь указать порядок скорости роста времени работы алгоритма при росте основного параметра задачи. Коэффициент перед функцией от N не важен:
    если требуется N^2 действие или 100 * N^2 действий - это всё равно будет O(N^2), потому что в обоих случаях рост N в два раза увеличивает время работы в четыре.
</p>

<p>"O"-обозначение используется, если нужно определить асимптотическую верхную границу для функции f(n),
    равную времени работы алгоритма в зависимости от основного параметра задачи.
{% highlight text %}
Определение взято из книги Томаса Кормена "Алгоритмы. Построение и анализ.".

Для данной функции g(n) обозначение O(g(n)) означает множество функций
    O(g(n)) = {f(n): существуют положительные константы c и n0,
                     такие что 0<=f(n)<=cg(n) для всех n>=n0}

{% endhighlight %}
    Также существует обозначение &Theta; (тета большое) - это асимптотическая граница сверху и снизу.
    Но оно не так популярно, потому что нет причин доказывать, что алгоритм работает не быстрее, чем, намного важнее, что он не медленнее, чем.
</p>

<p>Программисты не следуют педантично строгому определению и упрощают до примерно такого:
    f(n) = O(g(n)), если функция f(n) при больших n ведёт себя пропрционально функции g(n).
</p>

<p>На практике чаще всего встречаются пять функций определяющих сложность алгоритма:</p>
<dl class="dl-horizontal">
    <dt>O(1)</dt>
    <dd>Константа. Время работы фиксировано и не зависит от размера задачи.</dd>
    <dt>O(log(n))</dt>
    <dd>Логарифм. Основание логарифма опускается, потому что переход от одного основания к другому это умножение на константу.</dd>
    <dt>O(n)</dt>
    <dd>Линейная сложность. Время растёт пропорционально размеру задачи.</dd>
    <dt>O(n*log(n))</dt>
    <dd>Сложность, которая называется "Эн лог эн". Все хорошие алгоритмы сортировки имеют такую сложность.</dd>
    <dt>O(n*n)</dt>
    <dd>Квадратичкая сложность. Например, всевозможные попарные операции в массивах.</dd>
</dl>

<p>На графике ниже показана скорость роста этих функций в сравнении. Видно, что каждая следующая функия растёт намного быстрее предыдущей.
    Поэтому следует искать алгоритм, решающий задачу с наименьшей сложностью. Как правило, такой алгоритм сложнее и не всегда очевиден.
    На втором графике cравниваются две последние функции, чтобы показать, что между ними тоже существенное различие.</p>

<p>
<img class="code-snapshot" src="/img/posts/algorithm-complexity/perfcharts1.png" alt="картинки нет, но вы держитесь"/>
<img class="code-snapshot" src="/img/posts/algorithm-complexity/perfcharts2.png" alt="картинки нет, но вы держитесь"/>
</p>

<p>
    Java разработчику желательно знать несколько основных алгоритмов, понимать их сложность и область применения.
    Они собраны в таблице ниже.
</p>

<table class="table table-bordered">
    <thead>
    <tr>
        <th scope="col" class="alg-th">Алгоритм</th>
        <th scope="col">Лучший  случай</th>
        <th scope="col">Средний  случай</th>
        <th scope="col">Худший  случай</th>
        <th scope="col" class="comment-th">Комментарий</th>
    </tr>

    </thead>
    <tbody>
    <tr>
        <th scope="row">Получение элемента по индексу в массиве или ArrayList</th>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>Область памяти, выделенная под массив, непрерывна, а размер элемента фиксирован. </td>
    </tr>
    <tr>
        <th scope="row">Добавление элемента в конец ArrayList / LinkedList, в начало LinkedList</th>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>Для ArrayList требуется небольшое уточнение: периодически приходится расширять массив, когда он переполняется, и эта операция формально стоит O(n),
            хотя и используется очень эффективная функция System.arraycopy. Но так как расширение производится в полтора раза (int newCapacity = oldCapacity + (oldCapacity >> 1);),
            то общее количество таких расширений - логарифм, то есть пренебрежимо мало по сравнению с O(n).</td>
    </tr>
    <tr>
        <th scope="row">Добавление в середину, удаление из LinkedList</th>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>Такая производительность достигается только если уже есть ссылка на элемент (Iterator, ListIterator).
            Иначе нужно сначала найти элемент по индексу, что стоит O(n)</td>
    </tr>
    <tr>
        <th scope="row">Операции по ключу в HashMap</th>
        <td>O(1)</td>
        <td>O(1)</td>
        <td>O(n)</td>
        <td>Худший случай возникает имеет место скорее при плохой реализации hashCode, а не из-за специфичных данных.
        При хорошей функции hashCode достигается лишь в теории.</td>
    </tr>
    <tr>
        <th scope="row">Операции по ключу в TreeMap</th>
        <td>O(1)</td>
        <td>O(log(n))</td>
        <td>O(log(n))</td>
        <td>TreeMap реализован на основе <a href="https://ru.wikipedia.org/wiki/%D0%9A%D1%80%D0%B0%D1%81%D0%BD%D0%BE-%D1%87%D1%91%D1%80%D0%BD%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE">красно-чёрного дерева</a>.
            Лучший случай - если нашли элемент сразу на вершине дерева. Скорее теоретический случай, чем практический.</td>
    </tr>
    <tr>
        <th scope="row">Бинарный поиск</th>
        <td>O(1)</td>
        <td>O(log(n))</td>
        <td>O(log(n))</td>
        <td>Бинарный поиск по сути аналогичен поиску в сбалансированном бинарном дереве.</td>
    </tr>
    <tr>
        <th scope="row">Поиск в несортированном списке или массиве, ArrayList, LinkedList</th>
        <td>O(1)</td>
        <td>O(n)</td>
        <td>O(n)</td>
        <td>Нужно перебрать все элементы по очереди.</td>
    </tr>
    <tr>
        <th scope="row">Сортировка слиянием</th>
        <td>O(nlog(n))</td>
        <td>O(nlog(n))</td>
        <td>O(nlog(n))</td>
        <td>Также алгоритм требует O(n) дополнительной памяти. В OpenJDK используется <a href="https://ru.wikipedia.org/wiki/Timsort">TimSort</a> -
            "гибридный алгоритм сортировки, сочетающий сортировку вставками и сортировку слиянием". TimSort быстрее на частично отсортированных данных.</td>
    </tr>
    <tr>
        <th scope="row">Быстрая сортировка, она же QuickSort</th>
        <td>O(nlog(n))</td>
        <td>O(nlog(n))</td>
        <td>O(n^2)</td>
        <td>Не требует дополнительной памяти, кроме O(log n) для стека рекурсивных вызовов.</td>
    </tr>
    <tr>
        <th scope="row">Медленные аргоритмы сортировки: пузырьковая, вставками</th>
        <td>O(n^2)</td>
        <td>O(n^2)</td>
        <td>O(n^2)</td>
        <td></td>
    </tr>
    </tbody>
</table>


<p>
    Несколько слов об алгоритме быстрой сортировки (QuickSort). Сам алгоритм хорошо описан
    <a href="https://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D1%8B%D1%81%D1%82%D1%80%D0%B0%D1%8F_%D1%81%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0">здесь.</a>
    Алгоритм имеет пару нюансов понимания и реализации, без которых его производительность может падать до O(n^2),
    и алгоритм становится интересен лишь в академических целях, но не в практических.
</p>
<ol>
    <li>В качестве опорного элемента должен выбираться случайный элемент. Левый, правый или средний - не подходят.
        Тогда в среднем длина большей части массива будет в три раза больше меньшей (соотношение три к одному). То есть хуже, чем разбиение точно пополам,
        но длина цепочки рекурсивных вызовов всё равно будет логарифмом.
        Вероятность на каждой итерации случайно попадать в максимальный или минимальный элемент пренебрежимо мала
        (примерно 0.001^1000 для массива из тысячи элементов).

        В таком случае даже худшее время работы тоже будет O(nlog(n)), а не O(n^2), как принято говорить.
        Хотя это сугубо моё мнение, обычно говорят, что теоретически может быть квадратичная сложность, но на практике совсем редко.
    </li>
    <li>Следует выбирать версию алгоритма, которая делит подмассив на три части: меньшие опорного, равные ему, и большие.
        Классическое определение алгоритма разбивает на две части: меньшие либо равные и большие, но при этом делается предположение, что все элементы различны.
        Если пренебречь этой деталью и применить классическую реализацию к массиву равных элементов, то получим сложноть O(n^2).
        То есть дольше всего алгоритм работает когда и делать-то ничего не нужно! Если выделить элементы, равные опорному в отдельную группу, то для массивов
        с повторяющимися элементами наоборот получим существенный выигрыш.
    </li>
</ol>

<p>
    В реальных приложениях с качеством кода ниже среднего можно встретить неэффективную реализацию типовых задач. Например, пересечение двух коллекций можно посчитать за O(n),
    скопировав данные в HashSet, а на деле используется полный попарный перебор за O(n^2).
    Или же мы ищем данные в списке много раз за O(n), хотя можно отсортировать его один раз или же использовать TreeSet и тогда воспользоваться бинарным поиском за O(log(n)).
</p>

<p>Разберу одну задачу из типовых вопросов с собеседований. Постановка задачи такова: удалить из списка элементы с нечетными номерами, то есть каждый второй элемент.
    На <a href="https://stackoverflow.com/questions/15674960/remove-odd-elements-from-arraylist-while-iterating/15675082">
        stackoverflow есть страница</a> с похожим вопросом, хоть и старая и с небольшим количеством комментариев.
    Но ответы, которые предлагаются в качестве решений довольно показательны.
    Код на <a href="https://github.com/hipravin/samples-algorithm-complexity">GitHub</a>.
</p>

<p>
    К сожалению, в вопросе изначально уточняется, что удалить нужно из ArrayList ("Remove odd elements from ArrayList while iterating").
    Мы знаем, что удаление элемента из ArrayList приведёт к копированию всей хвостовой части, то есть будет стоить O(n), поэтому суммарная сложность будет O(n^2).
    Разумно же ожидать, что удаление будет произведено за один проход по списку, то есть O(n), и это легко достигается для LinkedList.
    Поэтому изначально использование ArrayList для такого сценария неверно.
    Если мы завязаны на ArrayList, то в реальном приложении стоит сделать копию массива и потом обновить ссылку.
    Если и это не применимо, то можно написать сложный код, который будет копировать элементы на пустые места, а потом удалит хвост, но всё-таки завершится за один проход по массиву.
</p>

<p>Работать со списком по индексам элементов неестественно. Обычно предикат будет зависеть от самих элементов списка, и тогда решение было бы таким:
{% highlight java %}
    //java8
    elements.removeIf(s -> Long.parseLong(s) % 2 == 1);
{% endhighlight %}
{% highlight java %}
    //before java8
    for (Iterator<String> iterator = elements.iterator(); iterator.hasNext(); ) {
        String next = iterator.next();
        if(Long.parseLong(next) % 2 == 1) {
            iterator.remove();
        }
    }
{% endhighlight %}
</p>

<p>Но на собеседованиях часто даются такие задачи с небольшим подвохом, чтобы проверить понимание нюансов.
    Решения исходной задачи:
{% highlight java %}
    //java8
    AtomicInteger counter = new AtomicInteger(0);
    elements.removeIf(s -> counter.incrementAndGet() % 2 == 0);
{% endhighlight %}
{% highlight java %}
    //before java8
    int counter = 0;
    for (Iterator<String> iterator = elements.iterator(); iterator.hasNext(); ) {
           iterator.next();
           if (counter++ % 2 == 1) {
              iterator.remove();
           }
    }
{% endhighlight %}
{% highlight java %}
    //before java8
    //хитрый способ избежать введения переменной counter. Следует избегать таких неочевидных приёмов.
    for (ListIterator<String> literator = elements.listIterator(elements.size()); literator.hasPrevious();) {
        literator.previous();
        if (literator.previousIndex() % 2 == 0) {
            literator.remove();
        }
    }
{% endhighlight %}
{% highlight java %}
    //O(n) for ArrayList
    //решение на крайний случай, если вероятны проблемы с производительностью, но от ArrayList не уйти.
    //объем кода не соответствует сложности задачи, читаемость его тоже низкая, придётся объяснять в комментариях,
    //почему так, а не иначе
    //если применить это решение к LinkedList, то получим O(n^2)

    removeFromArrayList(elements, i -> i % 2 == 1);

    static void removeFromArrayList(ArrayList<String> elements, IntPredicate removeIf) {
        int lastIndex = 0;
        for (int i = 0; i < elements.size(); i++) {
            if (!removeIf.test(i)) {
                elements.set(lastIndex, elements.get(i));
                lastIndex++;
            }
        }
        int extraElementsCount = elements.size() - lastIndex;
        //unfortunately, we can't use protected removeRange method
        for (int i = 0; i < extraElementsCount; i++) {
            elements.remove(elements.size() - 1);
        }
    }
{% endhighlight %}
</p>

<p>Прокомментирую решения, предложенные на stackoverflow.
{% highlight java %}
    //1.
    //Корректное решение. O(n^2) для ArrayList, O(n) для LinkedList
    int i = 0;
    for (Iterator<String> it = words.iterator(); it.hasNext(); )  {
        it.next(); // Add this line in your code
        if (i % 2 != 0) {
           it.remove();
        }
        i++;
    }
{% endhighlight %}
{% highlight java %}
    //2.
    //Некорректное решение. O(n^2) для ArrayList, O(n^2) для LinkedList.
    //Ошибочный результат при дублирующихся значениях в списке.
    int i = 0;
    List<String> list = new ArrayList<String>();
    for (String word:words)  {
        if (i % 2 != 0)   {
            //it.remove();
            list.add(word);
        }

        i++;
    }
    words.removeAll(list); //элементы удаляются по значению!
    //додумать решение можно так (ещё заменить условие на ==0):
    //words.clear();
    //words.addAll(list);
{% endhighlight %}
{% highlight java %}
   //3.
   //На этом примере я потерпел фиаско. Я был уверен, что код не работает, потому что размер и индексы будут изменяться по ходу работы алгоритма,
   //что, удаляя элемент с индексом 2, мы удаляем массив с индексом 3 в исходном массиве. Всё перепуталось и не может дать верный результат...
   //Но! результат верный. На каждом шагу мы удаляем элемент, сдвигаем все индексы на 1 влево, но увеличиваем счётчик на 1, а не на 2.
   //здесь мои полномочия всё, как говорится
   //Сложность O(n^2) для ArrayList и LinkedList

   int i = 1;

   while (i < words.size()) {
       words.remove(i++);
   }
{% endhighlight %}
{% highlight java %}
   //4.
   //Корректное решение. Единственное из предложенных O(n) для ArrayList; O(n^2) для LinkedList.
   //Минус - тяжело читаемый код. Два счетчика, называются i, j. Причём j всегда равно i/2.
   //Если это писали не Вы, то придётся запускать отладку, чтобы разобраться.
   int j = 0;
   for(int i = 0 ; i < integers.size(); i++){
       if( i % 2 == 0){
           integers.set(j, integers.get(i));
           j++;
       }
   }
   int half = integers.size()%2==0 ? integers.size()/2 : integers.size()/2 + 1;
   integers.subList(half , integers.size()).clear();//буду знать, что subList не создаёт новый список, а лишь ссылку на подсписок текущего.
           //поэтому хвост можно обрезать вот так в одну строчку.
{% endhighlight %}
</p>

<p>Подобным образом можно мусолить любые коллекции и операции с ними: LinkedList, ArrayList, HashMap, TreeMap, LinkedHashMap, CopyOnWriteArrayList и так далее.
Пример с удалением один из самых базовых и простых. В данном посте я этого делать не буду, перейду к итогу всего что я тут понаписал. </p>

<h4>Заключение</h4>
<p>Приходится соблюдать тонкий баланс между оптимальностью решения по производительности, читаемостью и лаконичностью кода и здравым смыслом.
Главное понимать насколько быстро можно решить типовую задачу и подобрать правильные структуры данных и операции с ними.
На интервью на позицию Java разработчика часто вопрос по данной тематике будет вторым (после "методов класса Object" или "расскажите о последнем проекте").
</p>
